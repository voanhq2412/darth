
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Convolutional Network Architecture &#8212; DARTS</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to your Jupyter Book" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">DARTS</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Convolutional Network Architecture
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/DARTS_paper_summary.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FDARTS_paper_summary.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/DARTS_paper_summary.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Convolutional Network Architecture
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#darts-overview">
   DARTS overview
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-methodology-and-approximation">
   Optimization Methodology and Approximation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#central-difference">
     Central Difference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deriving-discrete-architecture">
   Deriving Discrete Architecture
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recent-variants-of-darts">
   Recent variants of DARTS
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#idarts">
   iDARTS
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Convolutional Network Architecture</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Convolutional Network Architecture
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#darts-overview">
   DARTS overview
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-methodology-and-approximation">
   Optimization Methodology and Approximation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#central-difference">
     Central Difference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deriving-discrete-architecture">
   Deriving Discrete Architecture
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recent-variants-of-darts">
   Recent variants of DARTS
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#idarts">
   iDARTS
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <p>The performance of Machine Learning models are highly dependent on their hyperparameters. NN’s flexibility also means that there are more hyperparameters to tune, which may involve deciding the number of layers and the number of nodes per layer; tweaking these two hyperparameters can also be refered to as tweaking the architecture of the network.</p>
<p>Machine Learning models can be tweaked through trial and error, combined with intuition and experience. To be more precise and rigorous, one can model the architecture search problem as a discrete optimization problem. However, discrete optimization is much harder than continuous optimization, many NP-hard problems fall within this category.</p>
<p>Reinforcement Learning and Evolutionary Algorithms have been applied to search for good solutions to the discrete problem however they require a lot of computational resource. By modelling the problem as a continuous optimization problem, DARTS produce highly competive results in much less time when compared to the aforementioned non-differentiable search techniques.</p>
<section id="convolutional-network-architecture">
<h1>Convolutional Network Architecture<a class="headerlink" href="#convolutional-network-architecture" title="Permalink to this headline">#</a></h1>
<p><img alt="alt" src="_images/cnn-architecture.png" /></p>
<ul class="simple">
<li><p>The first part of the CNN is basically ‘feature extraction’, where convolutional filters (to extract feature(s) from the input) and pooling (to reduce the dimensions) are applied. These are repeated N number of times sequentially depending on the problem.</p></li>
<li><p>In the second part, output from the first part is flattened and fed into the usual artificial neural network for classification.</p></li>
</ul>
</section>
<section id="darts-overview">
<h1>DARTS overview<a class="headerlink" href="#darts-overview" title="Permalink to this headline">#</a></h1>
<p>DARTS optimizes one convolutional cell at a time. More precisely, it looks for the optimal sequence of operations within each cell. But what is a cell?</p>
<p>In DARTS, a cell basically takes inputs from the 2 previous layers, apply convolutional filters, pooling, concat and output to the next layer. The concatenation step is always at the end before obtaining the output. However, we want to search for the optimal selection and sequence of operations for the rest of the cell so our equations should incoporate some flexibility to allow for all the possible combinations.</p>
<p><img alt="alt" src="_images/nodes_sequence.jpg" /></p>
<ul class="simple">
<li><p>Within a cell, each intermediate node is computed based on all of the previous nodes.<br><br><br></p></li>
</ul>
<p><img alt="alt" src="_images/operation_weights.jpg" /></p>
<ul class="simple">
<li><p>Within each node, we assign probabilities to all the possible operations and select the operation with the highest probablity. This is how we turn the architecture search from a discrete into a continuous optimization problem.</p></li>
</ul>
<ol class="simple">
<li><p>Each of the <span class="math notranslate nohighlight">\(\alpha\)</span> stands for the mixing weight of an operation <span class="math notranslate nohighlight">\(o(x)\)</span>. So for operation <span class="math notranslate nohighlight">\(o_1\)</span> we would have weight <span class="math notranslate nohighlight">\(\alpha_1\)</span>, for operation <span class="math notranslate nohighlight">\(o_2\)</span> we have <span class="math notranslate nohighlight">\(\alpha_2\)</span>, so on and so forth.</p></li>
<li><p>We divide the <span class="math notranslate nohighlight">\(exp^{weight}\)</span> of a given operation by the sum of all <span class="math notranslate nohighlight">\(exp^{weight}\)</span> to obtain the probability of that operation being selected.</p></li>
<li><p>Also within each operation <span class="math notranslate nohighlight">\(o(x)\)</span> we have the weights of the convolutional filter, for a 3x3x3 filter that would be 27 weights.</p></li>
<li><p>We optimize the mixing weights of the operations and the convolutional weights with bilevel optimisation, such that the validation loss is minimized.</p></li>
</ol>
<p><img alt="alt" src="_images/DARTS_cell.jpeg" /></p>
</section>
<section id="optimization-methodology-and-approximation">
<h1>Optimization Methodology and Approximation<a class="headerlink" href="#optimization-methodology-and-approximation" title="Permalink to this headline">#</a></h1>
<p>Given a chosen architecture, to obtain the best CNN possible we’d still have to optimize the weights of our convolutional filters. Thus the outer optimization problem is one of choosing the optimal operations for a cell, the inner optimization problem is one of choosing the optimal weights for our convolutional filters. The outer problem is constrained by the inner problem.</p>
<p>The bilevel optimization is formulated as:<br>
<span class="math notranslate nohighlight">\(\underset{\alpha}{min} \ \ L_{val} (w^*(\alpha),\alpha)\)</span><br>
<span class="math notranslate nohighlight">\(s.t  \ \ \ \ w^*(\alpha) = \underset{w}{argmin} \ L_{train} (w,\alpha)\)</span></p>
<p>Mathematically, we could solve it using gradient descent …</p>
<ol class="simple">
<li><p>Given a random starting architecture <span class="math notranslate nohighlight">\(\alpha\)</span>, we want to search for the next best architecture <span class="math notranslate nohighlight">\(\alpha'\)</span>. Performing this improving search until convergence is the outer optimization problem. <br>
<span class="math notranslate nohighlight">\(\alpha' = \alpha - \lambda \nabla_{\alpha} L_{val}(w*,\alpha) \ \)</span>, where <span class="math notranslate nohighlight">\(w*\)</span>, the optimal weights of the architecture, is fixed<br></p></li>
<li><p>However, to obtain the optimal <span class="math notranslate nohighlight">\(w*\)</span>, we’d have to solve the inner optimization problem as well, updating w with …<br>
<span class="math notranslate nohighlight">\(w' = w - \xi \nabla_w L_{train}(w,\alpha) \ \ \ \)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is fixed<br></p></li>
</ol>
<p>For such a bilevel problem, for every <span class="math notranslate nohighlight">\(\alpha\)</span> iterate we’d have to solve the inner optimization problem to obtain <span class="math notranslate nohighlight">\(w*\)</span> such that <span class="math notranslate nohighlight">\(L_{train}\)</span> is minimized, then we return to update the <span class="math notranslate nohighlight">\(\alpha\)</span> for the outer problem. After that, we begin to search for a new set of optimal weights for the inner problem again given the new architecture. This procedure can be very time-consuming and impractical. <br></p>
<p>In addition, the inner problem and outer problem each by themselves is a nonconvex problem. Solving a non-convex problem to global convergence is already difficult so with a bilevel non-convex problem the difficulty is twofold.</p>
<hr class="docutils" />
<p>To reduce the difficulty, the author proposed to update the outer weights <span class="math notranslate nohighlight">\(\alpha\)</span> through an approximation scheme … <br><br>
<span class="math notranslate nohighlight">\(\nabla_\alpha L_{val} (w^*(a),a) \approx \nabla_\alpha L_{val} (w - \xi \nabla_w L_{train}(w,\alpha),a)\)</span> <br></p>
<ul class="simple">
<li><p>Rather than optimizing w to convergence then updating <span class="math notranslate nohighlight">\(\alpha\)</span>, we update both w and <span class="math notranslate nohighlight">\(\alpha\)</span> in one step. (this basically reduces the bilevel program to a single-level program)</p></li>
<li><p>Trading accuracy for speed means that we may (prematurely) move on to different architecture before knowing how much the current architecture with its optimal weights can minimize the validation loss.</p></li>
<li><p>As a result, the overall problem may not even reach a local optima. (however, the authors say that they’re able to reach a fixed point with a suitable value of <span class="math notranslate nohighlight">\(\xi\)</span>)</p></li>
</ul>
<p><a class="reference external" href="https://arxiv.org/pdf/1909.09656.pdf">https://arxiv.org/pdf/1909.09656.pdf</a> argues that this is not the cause of failure of the DARTS, the validation error progresses very well.</p>
<hr class="docutils" />
<p>By realizing that <span class="math notranslate nohighlight">\(w'\)</span> is a function of <span class="math notranslate nohighlight">\(\alpha\)</span>, we can include one additional gradient term to improve the approximation.</p>
<p><span class="math notranslate nohighlight">\(\nabla_x f(g(x),x) = \nabla_x f(g(x),x) + \nabla_{g(x)} f(g(x),x) . \nabla_x g(x)\)</span></p>
<p>Thus:<br>
<span class="math notranslate nohighlight">\(\nabla_\alpha L_{val} (w',a) \approx \nabla_\alpha L_{val} (w' ,a) - \nabla_{\alpha} w' . \nabla_{w'} L_{val}(w',\alpha) \ \ \ \ \)</span><br>
<span class="math notranslate nohighlight">\(\nabla_\alpha L_{val} (w',a) \approx \nabla_\alpha L_{val} (w' ,a) - \xi \nabla^2_{w\alpha} L_{train}(w,\alpha). \nabla_{w'} L_{val}(w',\alpha) \ \ \ \ \)</span><br></p>
<p>where …<br>
<span class="math notranslate nohighlight">\(w' = w - \xi \nabla_w L_{train}(w,\alpha) \)</span><br></p>
<p><span class="math notranslate nohighlight">\(\nabla_\alpha w' = - \xi \nabla^2_{w\alpha} L_{train}(w,\alpha)\)</span><br><br></p>
<p><span class="math notranslate nohighlight">\(\nabla^2_{w\alpha} L_{train}(w,\alpha)\)</span> is a matrix of second-order derivatives that is very expensive to compute. Finite difference is applied to approximate this term.</p>
<section id="central-difference">
<h2>Central Difference<a class="headerlink" href="#central-difference" title="Permalink to this headline">#</a></h2>
<p><span class="math notranslate nohighlight">\(f'(x) \approx \dfrac{f(x+h) - f(x-h)}{2h}\)</span>, as h –&gt; 0</p>
<p><span class="math notranslate nohighlight">\(\nabla^2_{w\alpha} L_{train}(w,\alpha) = \dfrac{\nabla_{\alpha}L_{train}(w+h,\alpha) - \nabla_{\alpha}L_{train}(w-h,\alpha)}{2h}\)</span></p>
<p>let <span class="math notranslate nohighlight">\(\epsilon\)</span> be a small scalar and <span class="math notranslate nohighlight">\(h = \epsilon\nabla_{w'} L_{val}(w',\alpha) \)</span></p>
<p><span class="math notranslate nohighlight">\(\nabla^2_{w\alpha} L_{train}(w,\alpha). \nabla_{w'} L_{val}(w',\alpha) = \dfrac{\nabla_{\alpha}L_{train}(w+\epsilon\nabla_{w'} L_{val}(w',\alpha),\alpha) - \nabla_{\alpha}L_{train}(w-\epsilon\nabla_{w'} L_{val}(w',\alpha),\alpha)}{2\epsilon\nabla_{w'} L_{val}(w',\alpha),\alpha)} .\nabla_{w'} L_{val}(w',\alpha),\alpha)\)</span>
<span class="math notranslate nohighlight">\(\qquad \qquad \qquad \qquad \qquad \qquad = \dfrac{\nabla_{\alpha}L_{train}(w+\epsilon\nabla_{w'} L_{val}(w',\alpha),\alpha) - \nabla_{\alpha}L_{train}(w-\epsilon\nabla_{w'} L_{val}(w',\alpha),\alpha)}{2\epsilon} \)</span></p>
</section>
</section>
<section id="deriving-discrete-architecture">
<h1>Deriving Discrete Architecture<a class="headerlink" href="#deriving-discrete-architecture" title="Permalink to this headline">#</a></h1>
<p>To discretize the architecture:</p>
<ul class="simple">
<li><p>We pick the most likely operation o(x) for each edge.</p></li>
<li><p>For each node, we pick the top k operations with the highest weights such that for each node there can only be k incoming edges. This can be considered a pruning step that removes weak operations.</p></li>
</ul>
<p>Requirement for Master Thesis:</p>
<ul class="simple">
<li><p>Main aim is to learn , not to create something new</p></li>
<li><p>Literature Review: Do a survey of recent papers/methods related to differentiable search, pick one paper/method and implement it on a different/new dataeset</p></li>
<li><p>Optimiztation algo is not main focus for differentiable search: focus on approximation</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="recent-variants-of-darts">
<h1>Recent variants of DARTS<a class="headerlink" href="#recent-variants-of-darts" title="Permalink to this headline">#</a></h1>
<p>Recent papers have been dedicated to avoid overfiting of DARTS to make it more robust.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Pros</p></th>
<th class="head"><p>Cons</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DARTS</p></td>
<td><p>Default</p></td>
<td><p>1) Weak-robustness, prone to performance collapse <br> because it accumulates paramter-free operations (especially skip-connections) (also, operations with fewer parameters are trained to optimality more quickly etc 3x3 kernels converge faster than 5x5 kernels and thus more likely to be picked in the final architecture <br> 2) Poor generalization of searched architecture due to overfitting</p></td>
</tr>
<tr class="row-odd"><td><p>DARTS+</p></td>
<td><p>1) Simple early stopping rule leads to performance increase over original DARTS. ie. the search is terminated once a certain number of skip connections have accumulated <br> 2) Prevents the build up of skip connections to avoid performance collapse</p></td>
<td><p>1) Early stopping rule may be considered arbitary and it may mistakenly rejects good architectures.</p></td>
</tr>
<tr class="row-even"><td><p>R-DARTS</p></td>
<td><p>1) Early stopping if the hessian’s largest eigenvalue of the inner objective gets too large.<br> 2) L2 regularization of the inner objective to control the eigenvalue.</p></td>
<td><p>1) Keeping track of the eigenvalue of the hessian can be costly. <br> 2) Reliance on the quality of the early stopping indicator.</p></td>
</tr>
<tr class="row-odd"><td><p>DROP-NAS</p></td>
<td><p>1) Dropping operations randomly so that operations with more parameters will be more likely to be in the final architecture. This reduces DARTS tendency to pick operations with less parameters because it takes less time for their weights to converege <br> 2) Operations are categorized into parameter vs non-parameter group and operations are dropped such that at least one operation remains in each group.</p></td>
<td><p>1) Randomization may lead to not picking the best operation for a given edge<br> 2) Extra parameter that requires tuning <br> 3) In the end, an edge still has to pick between a parameter operation and non-parameter operation. We know that DARTS tend to pick non-parameter operations, this could be a problem here.</p></td>
</tr>
<tr class="row-even"><td><p>P-DARTS</p></td>
<td><p>1) Progressively increase the network depth during search process so that by the end of it, the architecture is close to that which is used in evaluation, and thus provides better performance.<br> 2) Dropping operations to prevent overfitting and to increase speed</p></td>
<td><p>“Arbitary” dropping rules: Dropping operations for search-space approximation based on low weights (to prevent computational overhead associated with progressive search) and also operations dropout (to restrict the number of parameter-free operations) <br></p></td>
</tr>
<tr class="row-odd"><td><p>Single-DARTS</p></td>
<td><p>1) Formulate the optimization problem as a single-level optimization problem. Other papers have formulated the problem as a bi-level optimization problem but solved it approximately with a single-level problem anyways. One thus can argue that formulating it as a bi-level problem is redundant.<br> 2) good theoretical foundation, and simpler to implement the the bi-level problem<br> 3) Updates <span class="math notranslate nohighlight">\(\alpha\)</span> ad w on the same batch of data</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>Most papers (such as R-Darts) proposes the L2 regularization of the architecture mixing weight <span class="math notranslate nohighlight">\(\alpha\)</span>. L2 helps to make the model more sparse and exclude unimportant features.</p>
<p>The bilevel optimization is formulated as:<br>
<span class="math notranslate nohighlight">\(min_{\alpha} \ \ L_{val} (w^*(a),a) + \dfrac{r}{2}. a^T.a\)</span><br>
<span class="math notranslate nohighlight">\(s.t  \ \ \ \ \ w^*(a) = argmin_{w} \ L_{train} (w,a)\)</span></p>
<p>Gradient update<br>
<span class="math notranslate nohighlight">\(\alpha' = \alpha - \lambda \{ \nabla_{\alpha} L_{val}(w*,\alpha) + r. \alpha   \} \ \ \ \ \)</span> <br></p>
<p>where w*, the optimal weights of the architecture, is fixed;<br>
and r = regularization factor<br></p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="idarts">
<h1>iDARTS<a class="headerlink" href="#idarts" title="Permalink to this headline">#</a></h1>
<p>iDARTS <cite data-cite="zhang2021idarts"></cite></p>
<p><span id="id1">[<a class="reference internal" href="markdown.html#id3" title="Miao Zhang, Steven W Su, Shirui Pan, Xiaojun Chang, Ehsan M Abbasnejad, and Reza Haffari. Idarts: differentiable architecture search with stochastic implicit gradients. In International Conference on Machine Learning, 12557–12566. PMLR, 2021.">ZSP+21</a>]</span></p>
<p>as metnioned in</p>
<!-- BIBLIOGRAPHY START -->
<div class="csl-bib-body">
</div>
<!-- BIBLIOGRAPHY END --></section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Welcome to your Jupyter Book</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Anh Vo<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>